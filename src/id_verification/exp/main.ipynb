{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MARGIN = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "HANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "  hand_landmarks_list = detection_result.hand_landmarks\n",
    "  handedness_list = detection_result.handedness\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected hands to visualize.\n",
    "  for idx in range(len(hand_landmarks_list)):\n",
    "    hand_landmarks = hand_landmarks_list[idx]\n",
    "    handedness = handedness_list[idx]\n",
    "\n",
    "    # Draw the hand landmarks.\n",
    "    hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    hand_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
    "    ])\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "      annotated_image,\n",
    "      hand_landmarks_proto,\n",
    "      solutions.hands.HAND_CONNECTIONS,\n",
    "      solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "      solutions.drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "    # Get the top left corner of the detected hand's bounding box.\n",
    "    height, width, _ = annotated_image.shape\n",
    "    x_coordinates = [landmark.x for landmark in hand_landmarks]\n",
    "    y_coordinates = [landmark.y for landmark in hand_landmarks]\n",
    "    text_x = int(min(x_coordinates) * width)\n",
    "    text_y = int(min(y_coordinates) * height) - MARGIN\n",
    "\n",
    "    # Draw handedness (left or right hand) on the image.\n",
    "    cv2.putText(annotated_image, f\"{handedness[0].category_name}\",\n",
    "                (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
    "                FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
    "\n",
    "  return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('./image.jpg')\n",
    "# plot original image\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Import the necessary modules.\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# STEP 2: Create an HandLandmarker object.\n",
    "base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "options = vision.HandLandmarkerOptions(base_options=base_options,\n",
    "                                       num_hands=2)\n",
    "detector = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "# STEP 3: Load the input image.\n",
    "image = mp.Image.create_from_file(\"image.jpg\")\n",
    "\n",
    "# STEP 4: Detect hand landmarks from the input image.\n",
    "detection_result = detector.detect(image)\n",
    "\n",
    "# STEP 5: Process the classification result. In this case, visualize it.\n",
    "annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
    "# cv2_imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
    "plt.imshow(annotated_image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the two images\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(img)\n",
    "ax[0].axis('off')\n",
    "ax[0].set_title('Original Image')\n",
    "ax[1].imshow(annotated_image)\n",
    "ax[1].axis('off')\n",
    "ax[1].set_title('Annotated Image')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect the number of hands (Taiwanese Sign Language)\n",
    "data_dir = \"./chinese_number_gestures/\" # 0 ~ 9\n",
    "\n",
    "fig, ax = plt.subplots(2, 10, figsize=(20, 6))\n",
    "\n",
    "fig.suptitle('Detection of Taiwanese Sign Language Numbers', fontsize=16)\n",
    "\n",
    "for i in range(10):\n",
    "    img = Image.open(data_dir + str(i) + \".png\")\n",
    "    ax[0, i].imshow(img)\n",
    "    ax[0, i].axis('off')\n",
    "    ax[0, i].set_title(f'Original {i}')\n",
    "\n",
    "    image = mp.Image.create_from_file(data_dir + str(i) + \".png\")\n",
    "    detection_result = detector.detect(image)\n",
    "    \n",
    "    # Debug print to check detection result\n",
    "    print(f\"Detection result for image {i}: {detection_result}\")\n",
    "    \n",
    "    annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
    "    ax[1, i].imshow(annotated_image)\n",
    "    ax[1, i].axis('off')\n",
    "    ax[1, i].set_title(f'Annotated {i}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./real_case/teacher_wu/\" # 0 ~ 9\n",
    "\n",
    "fig, ax = plt.subplots(2, 10, figsize=(20, 6))\n",
    "\n",
    "fig.suptitle('Detection of Taiwanese Sign Language Numbers', fontsize=16)\n",
    "\n",
    "for i in range(10):\n",
    "    img = Image.open(data_dir + 'wu_' + str(i) + \".png\")\n",
    "    ax[0, i].imshow(img)\n",
    "    ax[0, i].axis('off')\n",
    "    ax[0, i].set_title(f'Original {i}')\n",
    "\n",
    "    image = mp.Image.create_from_file(data_dir + 'wu_' + str(i) + \".png\")\n",
    "    detection_result = detector.detect(image)\n",
    "    \n",
    "    # Debug print to check detection result\n",
    "    print(f\"Detection result for image {i}: {detection_result}\")\n",
    "    \n",
    "    annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
    "    ax[1, i].imshow(annotated_image)\n",
    "    ax[1, i].axis('off')\n",
    "    ax[1, i].set_title(f'Annotated {i}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_number3_loc = './chinese_number_gestures/3.png'\n",
    "real_case_number3_loc = './real_case/teacher_wu/wu_3.png'\n",
    "\n",
    "example_number3 = Image.open(example_number3_loc)\n",
    "real_case_number3 = Image.open(real_case_number3_loc)\n",
    "\n",
    "image_example_number3 = mp.Image.create_from_file(example_number3_loc)\n",
    "detection_result_example_number3 = detector.detect(image_example_number3)\n",
    "annotated_image_example_number3 = draw_landmarks_on_image(image_example_number3.numpy_view(), detection_result_example_number3)\n",
    "\n",
    "image_real_case_number3 = mp.Image.create_from_file(real_case_number3_loc)\n",
    "detection_result_real_case_number3 = detector.detect(image_real_case_number3)\n",
    "annotated_image_real_case_number3 = draw_landmarks_on_image(image_real_case_number3.numpy_view(), detection_result_real_case_number3)\n",
    "\n",
    "# compare the two images detection result\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(annotated_image_example_number3)\n",
    "ax[0].axis('off')\n",
    "ax[0].set_title('Example Image 3')\n",
    "ax[1].imshow(annotated_image_real_case_number3)\n",
    "ax[1].axis('off')\n",
    "ax[1].set_title('Real Case Image 3')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reg import HandLandmarkProcessor\n",
    "\n",
    "processor = HandLandmarkProcessor()\n",
    "\n",
    "basic_result_ex_num3 = processor.convert_to_array(detection_result_example_number3)\n",
    "basic_result_real_num3 = processor.convert_to_array(detection_result_real_case_number3)\n",
    "print(f\"Shape of basic_result_ex_num3: {basic_result_ex_num3[0].shape}\")\n",
    "print(f\"Shape of basic_result_real_num3: {basic_result_real_num3[0].shape}\")\n",
    "# print(basic_result)\n",
    "# print(basic_result[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(F.cosine_similarity(torch.tensor(basic_result_ex_num3[0]), torch.tensor(basic_result_real_num3[0]), dim=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new plot (real case teacher wu 0 ~ 9 detection and classification)\n",
    "\n",
    "data_dir = \"./real_case/teacher_wu/\" # 0 ~ 9\n",
    "\n",
    "fig, ax = plt.subplots(2, 10, figsize=(20, 8))\n",
    "\n",
    "fig.suptitle('Detection of Taiwanese Sign Language Numbers', fontsize=16)\n",
    "\n",
    "for i in range(10):\n",
    "    img = Image.open(data_dir + 'wu_' + str(i) + \".png\")\n",
    "    ax[0, i].imshow(img)\n",
    "    ax[0, i].axis('off')\n",
    "    ax[0, i].set_title(f'Original {i}')\n",
    "\n",
    "    image = mp.Image.create_from_file(data_dir + 'wu_' + str(i) + \".png\")\n",
    "    detection_result = detector.detect(image)\n",
    "\n",
    "    number_image = mp.Image.create_from_file(f\"./chinese_number_gestures/{i}.png\")\n",
    "    number_detection_result = detector.detect(number_image)\n",
    "    \n",
    "    # Debug print to check detection result\n",
    "    # print(f\"Detection result for image {i}: {detection_result}\")\n",
    "    \n",
    "    annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
    "    ax[1, i].imshow(annotated_image)\n",
    "    ax[1, i].axis('off')\n",
    "    ax[1, i].set_title(f'Annotated {i}')\n",
    "    \n",
    "    basic_result = processor.convert_to_array(detection_result)\n",
    "    basic_result_number = processor.convert_to_array(number_detection_result)\n",
    "    # print(f\"Shape of basic_result: {basic_result[0].shape}\")\n",
    "    # print(F.cosine_similarity(torch.tensor(basic_result[0]), torch.tensor(basic_result_number[0]), dim=1).mean())\n",
    "    # add confidence score to the image\n",
    "    ax[1, i].set_title(f'Annotated {i} \\n Confidence: {F.cosine_similarity(torch.tensor(basic_result[0]), torch.tensor(basic_result_number[0]), dim=1).mean():.2f} \\n Prediction: {i}')\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "class GestureSimilarityMatrix:\n",
    "    def __init__(self, processor, detector):\n",
    "        self.processor = processor\n",
    "        self.detector = detector\n",
    "        \n",
    "    def create_similarity_matrix(self, \n",
    "                               real_data_dir: str, \n",
    "                               example_data_dir: str, \n",
    "                               real_prefix: str = \"wu_\",\n",
    "                               num_gestures: int = 10) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        創建手勢相似度矩陣\n",
    "        \n",
    "        Args:\n",
    "            real_data_dir: 實際案例圖片目錄\n",
    "            example_data_dir: 範例圖片目錄\n",
    "            real_prefix: 實際案例圖片檔名前綴\n",
    "            num_gestures: 手勢數量\n",
    "            \n",
    "        Returns:\n",
    "            similarity_matrix: numpy array of shape (num_gestures, num_gestures)\n",
    "        \"\"\"\n",
    "        # 初始化相似度矩陣\n",
    "        similarity_matrix = np.zeros((num_gestures, num_gestures))\n",
    "        \n",
    "        # 計算所有組合的相似度\n",
    "        for i in range(num_gestures):\n",
    "            # 載入實際案例圖片\n",
    "            real_image = mp.Image.create_from_file(f\"{real_data_dir}{real_prefix}{i}.png\")\n",
    "            real_result = self.detector.detect(real_image)\n",
    "            real_landmarks = self.processor.convert_to_array(real_result)\n",
    "            \n",
    "            if real_landmarks is None:\n",
    "                print(f\"Warning: Failed to detect hand in real image {i}\")\n",
    "                continue\n",
    "                \n",
    "            for j in range(num_gestures):\n",
    "                # 載入範例圖片\n",
    "                example_image = mp.Image.create_from_file(f\"{example_data_dir}{j}.png\")\n",
    "                example_result = self.detector.detect(example_image)\n",
    "                example_landmarks = self.processor.convert_to_array(example_result)\n",
    "                \n",
    "                if example_landmarks is None:\n",
    "                    print(f\"Warning: Failed to detect hand in example image {j}\")\n",
    "                    continue\n",
    "                \n",
    "                # 計算相似度\n",
    "                similarity = F.cosine_similarity(\n",
    "                    torch.tensor(real_landmarks[0]),\n",
    "                    torch.tensor(example_landmarks[0]),\n",
    "                    dim=1\n",
    "                ).mean().item()\n",
    "                \n",
    "                similarity_matrix[i, j] = similarity\n",
    "                \n",
    "        return similarity_matrix\n",
    "    \n",
    "    def plot_similarity_matrix(self, \n",
    "                             similarity_matrix: np.ndarray,\n",
    "                             real_data_dir: str,\n",
    "                             example_data_dir: str,\n",
    "                             real_prefix: str = \"wu_\",\n",
    "                             figsize: tuple = (15, 10)):\n",
    "        \"\"\"繪製相似度矩陣熱圖和原始圖片\"\"\"\n",
    "        num_gestures = similarity_matrix.shape[0]\n",
    "        \n",
    "        # 創建主圖和軸\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        gs = plt.GridSpec(2, 2, height_ratios=[1, 2], width_ratios=[3, 1])\n",
    "        \n",
    "        # 上排顯示範例圖片\n",
    "        ax_examples = fig.add_subplot(gs[0, 0])\n",
    "        ax_examples.set_title(\"Example Gestures\")\n",
    "        ax_examples.axis('off')\n",
    "        \n",
    "        # 在上排橫向排列範例圖片\n",
    "        for i in range(num_gestures):\n",
    "            img = Image.open(f\"{example_data_dir}{i}.png\")\n",
    "            ax_img = fig.add_axes([0.1 + i*0.08, 0.75, 0.07, 0.15])  # 調整位置和大小\n",
    "            ax_img.imshow(img)\n",
    "            ax_img.axis('off')\n",
    "            ax_img.set_title(f'{i}')\n",
    "        \n",
    "        # 左側顯示實際案例圖片\n",
    "        ax_real = fig.add_subplot(gs[1, 1])\n",
    "        ax_real.set_title(\"Real Gestures\")\n",
    "        ax_real.axis('off')\n",
    "        \n",
    "        # 在左側縱向排列實際案例圖片\n",
    "        for i in range(num_gestures):\n",
    "            img = Image.open(f\"{real_data_dir}{real_prefix}{i}.png\")\n",
    "            ax_img = fig.add_axes([0.85, 0.7 - i*0.06, 0.07, 0.05])  # 調整位置和大小\n",
    "            ax_img.imshow(img)\n",
    "            ax_img.axis('off')\n",
    "            ax_img.set_title(f'{i}')\n",
    "        \n",
    "        # 繪製相似度矩陣熱圖\n",
    "        ax_heatmap = fig.add_subplot(gs[1, 0])\n",
    "        sns.heatmap(similarity_matrix, \n",
    "                   annot=True, \n",
    "                   fmt='.2f', \n",
    "                   cmap='YlOrRd',\n",
    "                   xticklabels=range(num_gestures),\n",
    "                   yticklabels=range(num_gestures),\n",
    "                   ax=ax_heatmap)\n",
    "        \n",
    "        ax_heatmap.set_title('Similarity Matrix')\n",
    "        ax_heatmap.set_xlabel('Example Gestures')\n",
    "        ax_heatmap.set_ylabel('Real Gestures')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 輸出每個手勢的最佳匹配\n",
    "        print(\"\\nBest matches for each gesture:\")\n",
    "        for i in range(num_gestures):\n",
    "            best_match = np.argmax(similarity_matrix[i])\n",
    "            print(f\"Real gesture {i} best matches with example {best_match} \"\n",
    "                  f\"(similarity: {similarity_matrix[i, best_match]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化比較器\n",
    "processor = HandLandmarkProcessor()\n",
    "detector = mp.tasks.vision.HandLandmarker.create_from_options(\n",
    "    mp.tasks.vision.HandLandmarkerOptions(\n",
    "        base_options=mp.tasks.BaseOptions(model_asset_path='hand_landmarker.task'),\n",
    "        num_hands=1))\n",
    "        \n",
    "# 創建相似度矩陣\n",
    "matrix_creator = GestureSimilarityMatrix(processor, detector)\n",
    "\n",
    "# 計算並繪製相似度矩陣\n",
    "similarity_matrix = matrix_creator.create_similarity_matrix(\n",
    "    real_data_dir=\"./real_case/teacher_wu/\",\n",
    "    example_data_dir=\"./chinese_number_gestures/\",\n",
    "    real_prefix=\"wu_\"\n",
    ")\n",
    "\n",
    "# 繪製結果\n",
    "matrix_creator.plot_similarity_matrix(\n",
    "    similarity_matrix,\n",
    "    real_data_dir=\"./real_case/teacher_wu/\",\n",
    "    example_data_dir=\"./chinese_number_gestures/\",\n",
    "    real_prefix=\"wu_\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
